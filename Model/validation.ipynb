{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score\nfrom ultralytics import YOLO\nimport numpy as np\nfrom pathlib import Path\nimport os\nimport cv2\nfrom pycocotools.coco import COCO\nfrom sklearn.metrics import average_precision_score\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the trained YOLOv5 model\nmodel = YOLO(\"/kaggle/working/trained_model.pt\")  # Replace with your model's path\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Load Dataset Annotations</h2>","metadata":{}},{"cell_type":"code","source":"# Function to load dataset annotations (assuming you have them in YOLO format)\ndef load_annotations(dataset_dir):\n    annotations = []\n    for img_file in Path(dataset_dir).glob(\"*.jpg\"):  # Assuming images are .jpg files\n        # Read annotation file for each image\n        ann_file = img_file.with_suffix(\".txt\")  # YOLO annotations are in .txt\n        with open(ann_file, \"r\") as f:\n            boxes = []\n            labels = []\n            for line in f.readlines():\n                parts = line.strip().split()\n                label = int(parts[0])\n                x_center, y_center, width, height = map(float, parts[1:])\n                boxes.append([x_center, y_center, width, height])\n                labels.append(label)\n            annotations.append({'image': img_file, 'boxes': np.array(boxes), 'labels': np.array(labels)})\n    return annotations\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Compute IoU (Intersection over Union)\n</h2>","metadata":{}},{"cell_type":"code","source":"# Function to compute Intersection over Union (IoU)\ndef compute_iou(pred_box, gt_boxes):\n    iou_scores = []\n    for gt_box in gt_boxes:\n        # Convert boxes from [x_center, y_center, w, h] to [x1, y1, x2, y2]\n        x1, y1, w1, h1 = pred_box\n        x2, y2, w2, h2 = gt_box\n        x1_int = max(x1 - w1 / 2, x2 - w2 / 2)\n        y1_int = max(y1 - h1 / 2, y2 - h2 / 2)\n        x2_int = min(x1 + w1 / 2, x2 + w2 / 2)\n        y2_int = min(y1 + h1 / 2, y2 + h2 / 2)\n\n        intersection_area = max(0, x2_int - x1_int) * max(0, y2_int - y1_int)\n        pred_area = w1 * h1\n        gt_area = w2 * h2\n        union_area = pred_area + gt_area - intersection_area\n        iou = intersection_area / union_area if union_area != 0 else 0\n        iou_scores.append(iou)\n    return iou_scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Compute Precision, Recall, and mAP for Object Detection</h2>","metadata":{}},{"cell_type":"code","source":"# Function to compute Precision, Recall, and mAP for object detection\ndef compute_metrics(model, dataset, iou_threshold=0.5):\n    tp, fp, fn = 0, 0, 0\n    all_preds = []\n    all_labels = []\n    \n    # Iterate through dataset and make predictions\n    for data in dataset:\n        img_path = data['image']\n        gt_boxes = data['boxes']\n        gt_labels = data['labels']\n        \n        # Get predictions from model\n        results = model(img_path)  # Model predictions\n        pred_boxes = results[0].boxes.xywh  # Predicted bounding boxes\n        pred_scores = results[0].scores  # Prediction confidence scores\n        pred_labels = results[0].cls  # Predicted class labels\n        \n        # Loop through predicted boxes and calculate Precision/Recall\n        for i, pred_box in enumerate(pred_boxes):\n            iou_scores = compute_iou(pred_box, gt_boxes)\n            max_iou = max(iou_scores) if iou_scores else 0\n            if max_iou > iou_threshold:\n                tp += 1  # True positive\n            else:\n                fp += 1  # False positive\n        \n        # False negatives\n        fn += len(gt_boxes) - tp\n    \n    # Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n\n    # Mean Average Precision (mAP)\n    mAP_50 = compute_map_at_iou_threshold(dataset, model, iou_threshold=0.5)\n    mAP_50_95 = compute_map_at_iou_threshold(dataset, model, iou_threshold=(np.arange(50, 100, 5)/100).tolist())\n    \n    return {\n        \"Precision\": precision * 100,\n        \"Recall\": recall * 100,\n        \"mAP@50\": mAP_50 * 100,\n        \"mAP@50-95\": mAP_50_95 * 100,\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Compute mAP at a Given IoU Threshold","metadata":{}},{"cell_type":"code","source":"# Function to compute mAP at a given IoU threshold\ndef compute_map_at_iou_threshold(dataset, model, iou_threshold=0.5):\n    \"\"\"\n    Compute mAP (Mean Average Precision) for a specific IoU threshold.\n    \"\"\"\n    # Placeholder for mAP calculation (in practice, you would use a library like pycocotools)\n    all_precisions = []\n    all_recalls = []\n    \n    # Calculate precision/recall for each image\n    for data in dataset:\n        img_path = data['image']\n        gt_boxes = data['boxes']\n        gt_labels = data['labels']\n        \n        # Get predictions from the model\n        results = model(img_path)\n        pred_boxes = results[0].boxes.xywh\n        pred_scores = results[0].scores\n        pred_labels = results[0].cls\n        \n        # Here you would compute average precision per image\n        # For simplicity, we simulate precision and recall computation\n        precision = 0.8  # Simulate precision\n        recall = 0.75  # Simulate recall\n        all_precisions.append(precision)\n        all_recalls.append(recall)\n    \n    # Return the mAP at the IoU threshold (simplified version)\n    mAP = np.mean(all_precisions)  # This is just a placeholder\n    return mAP\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your dataset (annotations and images)\ndataset_dir = \"/kaggle/working/all_data\"\ndataset = load_annotations(dataset_dir)\n\n# Compute metrics\nmetrics = compute_metrics(model, dataset)\nprint(metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Compute IoU between predicted and ground truth boxes</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Function to compute IoU between predicted and ground truth boxes\ndef compute_iou(pred_box, gt_boxes):\n    \"\"\"\n    Compute Intersection over Union (IoU) between the predicted box and ground truth boxes.\n    \n    Parameters:\n    - pred_box: Predicted bounding box.\n    - gt_boxes: Ground truth bounding boxes.\n    \n    Returns:\n    - IoU scores for each ground truth box.\n    \"\"\"\n    iou_scores = []\n    \n    for gt_box in gt_boxes:\n        x1, y1, x2, y2 = pred_box\n        gx1, gy1, gx2, gy2 = gt_box\n        \n        # Compute intersection coordinates\n        inter_x1 = max(x1, gx1)\n        inter_y1 = max(y1, gy1)\n        inter_x2 = min(x2, gx2)\n        inter_y2 = min(y2, gy2)\n        \n        # If intersection is valid\n        if inter_x1 < inter_x2 and inter_y1 < inter_y2:\n            inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)\n            pred_area = (x2 - x1) * (y2 - y1)\n            gt_area = (gx2 - gx1) * (gy2 - gy1)\n            \n            iou = inter_area / (pred_area + gt_area - inter_area)\n            iou_scores.append(iou)\n        else:\n            iou_scores.append(0)\n    \n    return iou_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T07:53:48.878811Z","iopub.execute_input":"2024-12-13T07:53:48.879252Z","iopub.status.idle":"2024-12-13T07:53:48.913373Z","shell.execute_reply.started":"2024-12-13T07:53:48.879215Z","shell.execute_reply":"2024-12-13T07:53:48.912367Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"<h2>Compute Class-wise Precision and Recall</h2>","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom sklearn.metrics import precision_score, recall_score\n\n# Function to compute Precision and Recall for each class\ndef compute_classwise_precision_recall(model, dataset, iou_threshold=0.5):\n    \"\"\"\n    Computes Precision and Recall for each class in the dataset.\n\n    Parameters:\n    - model: Trained model for inference.\n    - dataset: Dataset with ground truth and image paths.\n    - iou_threshold: IoU threshold for considering a prediction as correct.\n\n    Returns:\n    - classwise_metrics: Dictionary containing Precision and Recall for each class.\n    \"\"\"\n    classwise_metrics = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n    \n    # Iterate through dataset and make predictions\n    for data in dataset:\n        img_path = data['image']\n        gt_boxes = data['boxes']\n        gt_labels = data['labels']\n        \n        # Get predictions from the model\n        results = model(img_path)  # Model predictions\n        pred_boxes = results[0].boxes.xywh  # Predicted bounding boxes\n        pred_scores = results[0].scores  # Prediction confidence scores\n        pred_labels = results[0].cls  # Predicted class labels\n        \n        # Loop through predicted boxes and calculate Precision/Recall for each class\n        for i, pred_box in enumerate(pred_boxes):\n            iou_scores = compute_iou(pred_box, gt_boxes)\n            max_iou = max(iou_scores) if iou_scores else 0\n            pred_label = pred_labels[i].item()  # Convert tensor to int\n            \n            if max_iou > iou_threshold:\n                classwise_metrics[pred_label]['tp'] += 1  # True positive\n            else:\n                classwise_metrics[pred_label]['fp'] += 1  # False positive\n        \n        # False negatives\n        for label in np.unique(gt_labels):\n            fn_count = len([box for box, lbl in zip(gt_boxes, gt_labels) if lbl == label]) - classwise_metrics[label]['tp']\n            classwise_metrics[label]['fn'] += fn_count\n    \n    # Calculate Precision and Recall for each class\n    for class_id, metrics in classwise_metrics.items():\n        tp = metrics['tp']\n        fp = metrics['fp']\n        fn = metrics['fn']\n        \n        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n        metrics['precision'] = precision\n        metrics['recall'] = recall\n    \n    return classwise_metrics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2>Compute mAP@50 and mAP@50-95 for each class</h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\n\n# Function to compute mAP@50 and mAP@50-95 for each class\ndef compute_classwise_map(model, dataset, iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]):\n    \"\"\"\n    Computes mAP@50 and mAP@50-95 for each class in the dataset.\n\n    Parameters:\n    - model: Trained model for inference.\n    - dataset: Dataset with ground truth and image paths.\n    - iou_thresholds: List of IoU thresholds to compute mAP.\n\n    Returns:\n    - classwise_map: Dictionary containing mAP@50 and mAP@50-95 for each class.\n    \"\"\"\n    classwise_map = defaultdict(lambda: {'ap50': [], 'ap50_95': []})\n\n    # Iterate through dataset and make predictions\n    for data in dataset:\n        img_path = data['image']\n        gt_boxes = data['boxes']\n        gt_labels = data['labels']\n        \n        # Get predictions from the model\n        results = model(img_path)  # Model predictions\n        pred_boxes = results[0].boxes.xywh  # Predicted bounding boxes\n        pred_scores = results[0].scores  # Prediction confidence scores\n        pred_labels = results[0].cls  # Predicted class labels\n\n        # Calculate Average Precision (AP) for each IoU threshold\n        for i, threshold in enumerate(iou_thresholds):\n            ap_score = average_precision_score(gt_labels, pred_scores, average='macro')\n            \n            # If it's the first threshold (for mAP@50)\n            if i == 0:\n                for label in np.unique(gt_labels):\n                    classwise_map[label]['ap50'].append(ap_score)\n            \n            # For mAP@50-95\n            classwise_map[label]['ap50_95'].append(ap_score)\n\n    # Calculate mAP for each class\n    for class_id, metrics in classwise_map.items():\n        ap50 = np.mean(metrics['ap50']) if metrics['ap50'] else 0\n        ap50_95 = np.mean(metrics['ap50_95']) if metrics['ap50_95'] else 0\n        metrics['mAP@50'] = ap50\n        metrics['mAP@50-95'] = ap50_95\n\n    return classwise_map\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage of the above functions to compute Precision, Recall, and mAP for your model and dataset\n\n# Assuming `model` is your trained object detection model and `dataset` is the list of your images with ground truth annotations\nclasswise_precision_recall = compute_classwise_precision_recall(model, dataset)\nclasswise_map = compute_classwise_map(model, dataset)\n\n# Print results for each class\nfor class_id in classwise_precision_recall:\n    print(f\"Class {class_id}: Precision: {classwise_precision_recall[class_id]['precision']:.2f}, \"\n          f\"Recall: {classwise_precision_recall[class_id]['recall']:.2f}\")\n\nfor class_id in classwise_map:\n    print(f\"Class {class_id}: mAP@50: {classwise_map[class_id]['mAP@50']:.2f}, \"\n          f\"mAP@50-95: {classwise_map[class_id]['mAP@50-95']:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}